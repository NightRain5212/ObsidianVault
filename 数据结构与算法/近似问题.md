# 近似问题

* 即使某个问题存在有效算法， 好的近似算法也会发挥作用。
    * 因为待求解问题的实例是不确定的， 或者在一定程度上是不准确的， 如果使用近似算法造成的误差比不精确的数据带来的误差小， 并且近似算法远比精确算法高效， 那么， 出于实用的目的， 当然更愿意选择近似算法了。
* 近似算法的基本思想是**用近似最优解代替最优解**， 以换取算法设计上的简化和时间复杂度的降低。
* 近似算法是这样一个过程： **虽然它可能找不到一个最优解， 但它总会为待求解的问题提供一个解。**
* 为了具有实用性， 近似算法必须能够给出算法所产生的解与最优解之间的差别或者比例的一个界限， 它保证任意一个实例的近似最优解与最优解之间相差的程度。 显然， 这个差别越小， 近似算法越具有实用性。

* 衡量近似算法性能最重要的标准有两个：

    * (1) **算法的时间复杂度**： 近似算法的时间复杂度必须是多项式阶的， 这是设计近似算法的基本目标；

    * (2) **解的近似程度**： 近似最优解的近似程度也是设计近似算法的重要目标。 近似程度可能与近似算法本身、 问题规模， 乃至不同的输入实例都有关。

* 不失一般性， 假设近似算法求解的是最优化问题， 且对于一个确定的最优化问题， 每一个可行解所对应的目标函数值均为正数。

* 若一个最优化问题的**最优值**为 $c^*$， 求解该问题的一个近似算法求得的**近似最优值**为 $c$， 则将该近似算法的**近似比** (Approximate Ratio) $\eta$ 定义为：

$$\eta = \max \{ \frac{c}{c^*}, \frac{c^*}{c} \}$$

* 在通常情况下， 该性能比是问题输入规模 $n$ 的一个函数 $\rho(n)$， 即：

$$\max \{ \frac{c}{c^*}, \frac{c^*}{c} \} \le \rho(n)$$

* 这个定义对于最大化问题和最小化问题都是适用的。

    * 对于一个最大化问题， $c \le c^*$， 此时近似算法的近似比表示最优值 $c^*$ 比近似最优值 $c$ 大多少倍；
    
    * 对于一个最小化问题， $c^* \le c$， 此时近似算法的近似比表示近似最优值 $c$ 比最优值 $c^*$ 大多少倍。

	* 所以， 近似算法的近似比 $\eta$ **不会小于1**， **近似算法的近似比越大， 它求出的近似解就越差**。 显然， 一个能求得**最优解的近似算法， 其近似比为1。**

* 若一个最优化问题的最优值为 $c^*$， 求解该问题的一个近似算法求得的近似最优值为 $c$， 则将该近似算法的 **相对误差** (Relative Error) $\lambda$ 定义为：

$$\lambda = \left| \frac{c - c^*}{c^*} \right|$$

* 近似算法的相对误差**总是非负的**， 它表示一个近似最优解与最优解相差的程度。 若问题的输入规模为 $n$， 存在一个函数 $\epsilon(n)$， 使得

$$\left| \frac{c - c^*}{c^*} \right| \le \epsilon(n)$$

* 则称 $\epsilon(n)$ 为该近似算法的 **相对误差界** (Relative Error Bound)。
    近似算法的近似比 $\rho(n)$ 与相对误差界 $\epsilon(n)$ 之间显然有如下关系：

$$\epsilon(n) \le \rho(n) - 1$$

